# Software Engineering

## #12 Accessibility & Software Evaluation ‚úÖ

WSEG / HS24<br />

[Source](https://github.com/digital-sustainability/module-wseg/tree/24/hs/docs/slides/content/12) licensed under [CC-BY-4.0](https://github.com/digital-sustainability/module-wseg/blob/24/hs/LICENSE)

--

0. R√ºckblick
1. Accessibilty (HAMK Finland)
2. Software Evaluation
3. Ausf√ºllen BFH-Evaluation
4. Ausblick
5. Coaching
6. 15.00 Gastvortrag Puzzle

--

### Offene Fragen?

#### Deliverable 4

- README anpassen, Vorlage folgt
- CI/CD: Applikation per "npm build" auf Gitlab-Pages ver√∂ffentlichen

---

### Accessibility

https://www.adis.ch/de/home-1.html

#### PDF-Folien von der HAMK üá´üáÆ

- Double Degree Programme (tommi.saksa@hamk.fi)

Note:
Blind person: https://www.youtube.com/watch?v=q_ATY9gimOM
WCAG: https://www.youtube.com/watch?v=5H1JGdqLrWo

---

### Begriff: "Software Evaluation"

> (Fast) fertige Produkte oder Anleitungen auf Usability optimieren

Ziele:

- Bessere Wettbewerbsf√§higkeit
- Steigerung der Wertsch√∂pfung beim Kunden
- Senkung anfallender Kosten beim Hersteller
  - Beispiel Transline (100k f√ºr Dokumentation)

Note:
√úbersetzungssoftware: 30% Senkung der Hotlinekosten um 2 Mio
https://www.transline.de/wissenswertes/kostensenkung-durch-bessere-dokumentation/

--

### Versus: Nutzungsforschung / Gestaltung

- Persona / Use Cases / Marktanalyse
- Card Sorting / Wording
- Prototyping<br />&nbsp;

#### User-Centered Design ab Beginn

F√ºr Usability-Probleme gilt auch die "Rule of ten"!

--

### Qualitative Methoden zur Evaluation

- Expert\*innen
- Nutzer\*innen

--

### Experts

- Heuristic Evaluation
  - Erfahrungsbasiert (z.B. [Nielsen](https://www.nngroup.com/articles/ten-usability-heuristics/))
  - Kriterienkataloge (ISO 9241-110)
- [Cognitive Walkthrough](https://de.wikipedia.org/wiki/Cognitive_Walkthrough)
  - Schwachstellen in der Interaktion mit UI entdecken

---

#### Users

> Mit 5 Testpersonen (pro Persona) findet man 80% der Erkenntnisse √ºber Usability-Probleme.

- Feldtest
- Usability-Test: Labortest / Remote
  - mit Eyetracking
- Usability-Walkthrough
- Quantitativ: A/B-Test, [SUS](https://de.wikipedia.org/wiki/System_Usability_Scale) etc.
- Benutzertagebuch

--

### Methode: Feldtest

- heimliches beobachten
- anschliessend befragen

#### Vorteile

- Keine Simulation, keine Beeinflussung
- Nutzende sind unbelastet / unvoreingenommen
- neue Ideen duch unerwartete Situationen

#### Nachteile

- Fortgeschrittener Prototyp notwendig
- Zufallsverteilung

--

### Methode: Labortest

- vorgegebene Aufgaben l√∂sen
- Beobachter in separatem Raum
- ggf. auch remote durchf√ºhrbar

#### Vorteile

- wenig Beeinflussung/Ablenkung
- geringe Versuchsabweichung

#### Nachteile

- zeit-/kostenintensiv
- Szenario f√ºhrt Testperson

--

### Methode: Usability-Walkthrough

- Testperson nutzt Produkt (Website, App...), denkt laut und kommentiert.
- Moderator\*in stellt R√ºckfragen, wo n√∂tig.
- Abschliessendes Interview
- auch remote durchf√ºhrbar

--

#### Vorteile

- Individuelle Anpassung an Testperson (Rhythmus, Aufgaben)
- Direkter Einfluss auf gewisse Probleme m√∂glich, keine ¬´Sackgassen¬ª
- Intensive Diskussion m√∂glich

#### Nachteile

- reale Nutzungssituation kaum simuliert
- Hohe Moderationskompetenz notwendig

--

## Fragen?

<!-- .slide: data-background="#fff5c1" -->

---

### Ausblicke

- Modulevaluation (= per Mail) ausf√ºllen ‚úçüèº
- N√§chste Woche #13:
  - "Wie wird ein Projekt abgeschlossen" (MT)
  - Reihenfolge Pr√§sentationen festlegen / Technisches Setup
  - Testfahrt Lernstick-Pr√ºfungsumgebung mit [CAMPLA](https://campla.github.io/)
- Letztes Coaching/Vorbereitung

---

- #14 "Abschluss-Pr√§sentationen" (Greenfield)
  - Agile Aktivit√§t
  - Nachbesprechung Evaluationsergebnisse
  - Fragen zur Pr√ºfung
  - ab xx.yy Uhr, festgelegte Gruppenreihenfolge
